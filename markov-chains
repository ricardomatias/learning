### Definition

A Markov chain tells you the probability of hopping, or "transitioning," from one state to any other state.

#### Example
*If you made a Markov chain model of a baby's behavior, you might include "playing," "eating", "sleeping," and "crying" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, ---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.*